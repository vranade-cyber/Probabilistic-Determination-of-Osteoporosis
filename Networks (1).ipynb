{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L49PKnYukgb1"
      },
      "source": [
        "Author-VIRAJ RANADE\n",
        "CV-24678 Project\n",
        "This Notebook is made as a toolbox to try and run the results of our Analysis. These networks will be used on the pre-processed Images to obtain a result."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "k573Q30Gh_3s"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import cv2\n",
        "import PIL\n",
        "import glob\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, utils\n",
        "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "oEU2H7amiCHx"
      },
      "outputs": [],
      "source": [
        "normal='/content/drive/MyDrive/CV_project/normal/*.*'\n",
        "osteoporosis='/content/drive/MyDrive/CV_project/osteoporosis/*.*'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "IU3G8049iHpX"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "r5u9irQ6iNXu"
      },
      "outputs": [],
      "source": [
        "def create_dataset(path1,path2):\n",
        "  set_1=glob.glob(path1)\n",
        "  set_2=glob.glob(path2)\n",
        "\n",
        "  s1=[]\n",
        "  s2=[]\n",
        "  lab1=np.zeros((186))\n",
        "  lab2=np.ones((186))\n",
        "\n",
        "  for i in set_1:\n",
        "    temp1=cv2.imread(i)\n",
        "    gray1=cv2.cvtColor(temp1,cv2.COLOR_BGR2GRAY)\n",
        "    gray_scaled1=cv2.resize(gray1,(156,235)).flatten()\n",
        "    s1.append(gray_scaled1)\n",
        "    #lab.append(0)\n",
        "\n",
        "  for i in set_2:\n",
        "    temp2=cv2.imread(i)\n",
        "    gray2=cv2.cvtColor(temp2,cv2.COLOR_BGR2GRAY)\n",
        "    gray_scaled2=cv2.resize(gray2,(156,235)).flatten()\n",
        "    s2.append(gray_scaled2)\n",
        "    #lab.append(1)\n",
        "\n",
        "  s1=np.asarray(s1)\n",
        "  s2=np.asarray(s2)\n",
        "  data_2=np.concatenate((s1,s2),axis=0)\n",
        "  lab=np.concatenate((lab1,lab2),axis=0)\n",
        "  print(type(data_2))\n",
        "  return data_2,lab\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q6DUaXGkiNt4"
      },
      "outputs": [],
      "source": [
        "data_20,labels_20=create_dataset(normal,osteoporosis)\n",
        "print(data_20.shape)\n",
        "print(labels_20.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ByYOnBfJiNw_"
      },
      "outputs": [],
      "source": [
        "import sklearn\n",
        "import sklearn.decomposition as dec\n",
        "from sklearn import model_selection\n",
        "train_2,test_2,train_labels_2,test_labels_2=sklearn.model_selection.train_test_split(data_20,labels_20,test_size=0.15)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kpMbhfUliNzt"
      },
      "outputs": [],
      "source": [
        "class MyDataSet(Dataset):\n",
        "  def __init__(self,x,y):\n",
        "    self.x=x\n",
        "    self.y=y\n",
        "    #self.length=np.shape(self.data)[0]\n",
        "    self.length=len(self.x)\n",
        "    print(self.length)\n",
        "\n",
        "  def __len__(self):\n",
        "    return self.length\n",
        "\n",
        "  def __getitem__(self,index):\n",
        "    xx=torch.FloatTensor(self.x[index])\n",
        "    yy=torch.FloatTensor(self.y[index])\n",
        "    return xx,yy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OQxFhVB2iN2l"
      },
      "outputs": [],
      "source": [
        "train_set=MyDataSet(train_2,train_labels_2.reshape(316,1))\n",
        "train_args=dict(shuffle=True,batch_size=32,num_workers=1,collate_fn=None)\n",
        "train_loader=DataLoader(train_set,**train_args)\n",
        "\n",
        "val_set=MyDataSet(test_2,test_labels_2.reshape(56,1))\n",
        "val_args=dict(shuffle=False,batch_size=32,num_workers=1,collate_fn=None)\n",
        "val_loader=DataLoader(val_set,**val_args)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dZ-RiAGoiN5O"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bGjtoU7fiN72"
      },
      "outputs": [],
      "source": [
        "class MLP(nn.Module):\n",
        "  def __init__(self,input_size,hiddens,output_size):\n",
        "    super(MLP,self).__init__()\n",
        "    Linear_layers=[nn.Linear(input_size,hiddens[0],bias=True),nn.BatchNorm1d(hiddens[0]),nn.ReLU(inplace=True),nn.Dropout(0.5)]\n",
        "    for i in range(len(hiddens)-1):\n",
        "      Linear_layers.append(nn.Linear(hiddens[i],hiddens[i+1],bias=True))\n",
        "      Linear_layers.append(nn.BatchNorm1d(hiddens[i+1]))\n",
        "      Linear_layers.append(nn.ReLU(inplace=True))\n",
        "      Linear_layers.append(nn.Dropout(0.5))\n",
        "    Linear_layers.append(nn.Linear(hiddens[i+1],output_size))\n",
        "\n",
        "    self.layers=nn.Sequential(*Linear_layers)\n",
        "\n",
        "  def forward(self,x):\n",
        "    return self.layers(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Klf7oIliN-f"
      },
      "outputs": [],
      "source": [
        "input=47\n",
        "hiddens=[512,1024,512,256,128]\n",
        "output=2\n",
        "steps=[x for x in np.arange(10)*5]\n",
        "print(steps)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nEMgqa3ZiOBO"
      },
      "outputs": [],
      "source": [
        "model=MLP(input,hiddens,output)\n",
        "criterion=nn.CrossEntropyLoss()\n",
        "optimizer=optim.Adam(model.parameters(),lr=1e-3,weight_decay=1e-4)\n",
        "scheduler=optim.lr_scheduler.MultiStepLR(optimizer,steps)\n",
        "\n",
        "device=torch.device('cuda' if torch.cuda else 'cpu')\n",
        "model=model.to(device)\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pmtBk-fTiOD_"
      },
      "outputs": [],
      "source": [
        "class Block(nn.Module):\n",
        "  def __init__(self,in_channels,hidden_channels,out_channels,kernel_size=3,stride=1):\n",
        "    self.stride=stride\n",
        "    super().__init__()\n",
        "\n",
        "    self.in_channel=in_channels\n",
        "    self.hidden_channels=hidden_channels\n",
        "    self.out_channels=out_channels\n",
        "\n",
        "    self.b=nn.Sequential(nn.Conv2d(self.in_channel,self.hidden_channels),nn.BatchNorm2d(self.hidden_channels),nn.ReLU,\n",
        "                         nn.Conv2d(self.hidden_channels,self.hidden_channels),nn.BatchNorm2d(self.hidden_channels),nn.ReLU,\n",
        "                         nn.Conv2d(self.hidden_channels,self.out_channels),nn.BatchNorm2d(self.out_channels))\n",
        "    \n",
        "    if self.stride==1:\n",
        "      self.s=nn.Identity()\n",
        "    else:\n",
        "      self.s=nn.Sequential(nn.Conv2d(self.in_channel,self.out_channels,kernel_size=3,stride=self.stride,padding=1),nn.BatchNorm2d(self.out_channels))\n",
        "    \n",
        "  def forward(self,x,flag):\n",
        "\n",
        "    embedding=self.b(x)\n",
        "    shortcut=self.s(x)\n",
        "    if flag:\n",
        "      self.out=nn.ELU(embedding+shortcut)\n",
        "    else:\n",
        "      self.out=nn.ReLU(embedding+shortcut)\n",
        "    \n",
        "    return self.out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iEkulMn8iOGu"
      },
      "outputs": [],
      "source": [
        "class CNN(nn.Module):\n",
        "  def __init__(self,in_features,num_classes,feat_dim=1):\n",
        "    super().__init__()\n",
        "    self.convolutional_layers=nn.Sequential(\n",
        "        nn.Conv2d(in_features,64,kernel_size=3,stride=2,bias=False),\n",
        "        nn.BatchNorm2d(64),\n",
        "        nn.ReLU(),\n",
        "\n",
        "        Block(64,128,64),\n",
        "        Block(64,128,64),\n",
        "        Block(64,128,128,3,2),\n",
        "        Block(128,256,128),\n",
        "        Block(128,256,128),\n",
        "        Block(128,256,256,3,2),\n",
        "        Block(256,512,512),\n",
        "        Block(512,1024,1024,3,2),\n",
        "\n",
        "        nn.AdaptiveAvgPool2d((1,1)),\n",
        "        nn.Flatten()\n",
        "    )\n",
        "\n",
        "    self.linear_layers=nn.Sequential(nn.Linear(1024,512),nn.BatchNorm1d(512),nn.ReLU(inplace=True),nn.Dropout(0.5),\n",
        "                nn.Linear(512,256),nn.BatchNorm1d(256),nn.ReLU(inplace=True),nn.Dropout(0.5),\n",
        "                nn.Linear(256,128),nn.BatchNorm1d(128),nn.ReLU(inplace=True),nn.Dropout(0.5),\n",
        "                nn.Linear(128,32),nn.BatchNorm1d(32),nn.ReLU(inplace=True),nn.Dropout(0.5),\n",
        "                nn.Linear(32,2)\n",
        "    )\n",
        "\n",
        "  def forward(self,x,return_embedding=False):\n",
        "    embedding=self.convolutional_layers(x)\n",
        "    output=self.linear_layers(embeddings)\n",
        "    if return_embedding:\n",
        "      return embedding,output\n",
        "    else:\n",
        "      return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IEqYrwcXjP9v"
      },
      "outputs": [],
      "source": [
        "network=CNN(47,2,1)\n",
        "network.to(device)\n",
        "\n",
        "criterion=nn.CrossEntropyLoss()\n",
        "optimizer=optim.Adam(model.parameters(),lr=1e-3,weight_decay=1e-4)\n",
        "scheduler=optim.lr_scheduler.MultiStepLR(optimizer,steps)\n",
        "\n",
        "print(network)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZYbA1l7LjQAd"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I8R-TlnsjQDM"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_46iuN-wjQGE"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Networks.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
